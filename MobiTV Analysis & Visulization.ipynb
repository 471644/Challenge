{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-Libraries\" data-toc-modified-id=\"Load-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load Libraries</a></span></li><li><span><a href=\"#Display-Settings\" data-toc-modified-id=\"Display-Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Display Settings</a></span></li><li><span><a href=\"#Utility-Functions\" data-toc-modified-id=\"Utility-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Utility Functions</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Load Data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import decimal\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Settings ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_colwidth',999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Function To Find Identical Columns #######\n",
    "\n",
    "def identical_columns(df, return_dataframe = False, verbose = False):\n",
    "    '''\n",
    "        a function to detect and possibly remove duplicated columns for a pandas data frame\n",
    "    '''\n",
    "    \n",
    "    # group columns by d-types, only the columns of the same d-types can be duplicate of each other\n",
    "    groups = df.columns.to_series().groupby(df.dtypes).groups\n",
    "    duplicated_columns = []\n",
    " \n",
    "    for dtype, col_names in groups.items():\n",
    "        column_values = df[col_names]\n",
    "        num_columns = len(col_names)\n",
    " \n",
    "        # find duplicated columns by checking pairs of columns, store first column name if duplicate exist \n",
    "        for i in range(num_columns):\n",
    "            column_i = column_values.iloc[:,i].values\n",
    "            for j in range(i + 1, num_columns):\n",
    "                column_j = column_values.iloc[:,j].values\n",
    "# or np.array_equal(column_i, column_j)\n",
    "                if np.array_equiv(column_i, column_j):\n",
    "                    if verbose: \n",
    "                        print(\"column {} is a duplicate of column {}\".format(col_names[i], col_names[j]))\n",
    "                    duplicated_columns.append(col_names[j])\n",
    "                    break\n",
    "    if not return_dataframe:\n",
    "        # return the column names of those duplicated exists\n",
    "        return duplicated_columns\n",
    "    else:\n",
    "        # return a data frame with duplicated columns dropped \n",
    "        return df.drop(labels = duplicated_columns, axis = 1)\n",
    "\n",
    "###########################################################################################\n",
    "## Outliers Detection ##\n",
    "# used extend instead of append because it will add the list's elements not the whole list.\n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"Takes a dataframe df of features and return a list of the indices corresponding to the observations \n",
    "    containing more than n outliers according to the Turkey method\"\"\"\n",
    "    outlier_indices = []\n",
    "    # iterate over feature(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col],25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3-Q1\n",
    "        # outiler step \n",
    "        outlier_step = 1.5*IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col]< Q1 - outlier_step)|(df[col] > Q3 + outlier_step)].index\n",
    "        \n",
    "        #Append the found outlier indices for col to the List of outliers indices \n",
    "        # used extend instead of append because it will add the list's elements not the whole list.\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "        # select observations containing more than 2 outliers \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k,v in outlier_indices.items() if v > n)\n",
    "        \n",
    "    return multiple_outliers\n",
    "\n",
    "##############################################################################################\n",
    "## Remove Features with zero variance\n",
    "def removeZeroVarianceData(df):\n",
    "\n",
    "\n",
    "    df_ = pd.DataFrame({'columnNames':df.apply(lambda x : len(x.unique().tolist())).index,\n",
    "                  'uniquelength' : df.apply(lambda x : len(x.unique().tolist())).values})    \n",
    "    \n",
    "    return df_[df_.uniquelength < 2].columnNames.tolist()\n",
    "\n",
    "######################################################################################################\n",
    "# Get columns with missing values more than missingValThreshold\n",
    "def getColumnWithMissingValues(df, missingValThreshold):\n",
    "    countMissing = df.isnull().sum()\n",
    "    listOfColumns = countMissing[countMissing > missingValThreshold]\n",
    "    return pd.DataFrame({'Columns with Missing Values': listOfColumns.index})\n",
    "\n",
    "##################################################################################################################################################################\n",
    "# Extract all date columns from dataframe\n",
    "def extract_datetime_cols(df):\n",
    "    date_col = []\n",
    "    for col in df.columns:\n",
    "        if (\"Date\" in col) and (\"_S\" not in col) and (\"_\" not in col) and (\"_MISS\" not in col):\n",
    "            date_col.append(col)\n",
    "    return date_col\n",
    "\n",
    "##################################################################################################################################################################\n",
    "#extract datetime features from transaction datetime\n",
    "def create_datetime_features(df,col):\n",
    "    dummy = pd.DataFrame()\n",
    "    col = col\n",
    "\n",
    "    dummy[col + \"_year\"] = df[col].dt.year\n",
    "    dummy[col + \"_month\"] = df[col].dt.month\n",
    "    dummy[col + \"_day\"] = df[col].dt.day\n",
    "    dummy[col + \"_weekday\"] = df[col].dt.weekday\n",
    "    dummy[col + \"_dayofweek\"] = df[col].dt.dayofweek\n",
    "    dummy[col + \"_quarter\"] = df[col].dt.quarter\n",
    "    dummy[col + \"_weekofyear\"] = df[col].dt.weekofyear\n",
    "    dummy[col + \"_week\"] = df[col].dt.week\n",
    "    dummy[col + \"_is_month_start\"] = df[col].dt.is_month_start.astype(int)\n",
    "    dummy[col + \"_is_month_end\"] = df[col].dt.is_month_end.astype(int)\n",
    "    dummy[col + \"_hour\"] = df[col].dt.hour\n",
    "    dummy[col + \"_minute\"] = df[col].dt.minute\n",
    "    return pd.concat([df,dummy],axis=1)\n",
    "################################################################################################################################################################\n",
    "## Extract Type ##\n",
    "\n",
    "def getType(l, name, not_numeric_ind_, maxCategories, origType):\n",
    "    not_numeric_ind = not_numeric_ind_.bool()\n",
    "    if 'Key' in name:\n",
    "        return 'key'\n",
    "    if 'date' in name:\n",
    "        return 'date'\n",
    "    if l == 0:\n",
    "        return 'only_nulls'\n",
    "    if l == 1:\n",
    "        return 'one_value'\n",
    "    if (l == 2) and (not_numeric_ind == True):\n",
    "        return 'binary_string'\n",
    "    if (l == 2) and (not_numeric_ind == False):\n",
    "        return 'binary_value'\n",
    "    if (l > 2 and l <= maxCategories) & (not_numeric_ind == True):\n",
    "        return 'categorical_string'\n",
    "    if (l > 2 and l <= maxCategories) & (not_numeric_ind == False):\n",
    "        return 'categorical_number'\n",
    "    if (l > maxCategories) & (not_numeric_ind == True):\n",
    "        return 'string'\n",
    "    if (l > maxCategories) & (not_numeric_ind == False):\n",
    "        return origType\n",
    "    \n",
    "def getDataTypeTable(df, maxCategories):\n",
    "    typesSeries = df.dtypes\n",
    "    types = pd.DataFrame({'column':typesSeries.index, 'originalType':typesSeries.values})\n",
    "    data_types = []\n",
    "\n",
    "    for name, col in df.iteritems():\n",
    "        l = len(col.unique())\n",
    "        origType = types[types['column']==name]['originalType']\n",
    "        not_numeric_ind = (origType == 'character') | ((origType != 'int64') & (origType != 'float64'))\n",
    "        data_types.append(getType(l, name, not_numeric_ind, maxCategories, origType))\n",
    "\n",
    "    types['CalculatedData_type'] = data_types\n",
    "    return types\n",
    "################################################################################################################################################################\n",
    "\n",
    "##################################################################################################################################################################\n",
    "\n",
    "#Impute Missing values ( Mode,Median,Mean)\n",
    "def impute_missing(data):\n",
    "    for col in data.columns:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            if data[col].dtype == \"object\":\n",
    "                data[col] = data[col].fillna(\"Missing\")\n",
    "            else:\n",
    "                if (abs(data[col].median()-data[col].mean())/data[col].mean()) > 0.1:\n",
    "                    data[col] = data[col].fillna(data[col].median())\n",
    "                else:\n",
    "                    data[col] = data[col].fillna(data[col].mean())\n",
    "    return data\n",
    "\n",
    "\n",
    "##################################################################################################################################################################\n",
    "\n",
    "def diff_between_trx_other_dates_in_days(df,transaction_datetime):\n",
    "    date_cols = extract_datetime_cols(df)\n",
    "    dummy = pd.DataFrame()\n",
    "    for col in date_cols:\n",
    "        if col != transaction_datetime:\n",
    "            dummy[col + \"_diff\"] = abs((df[col] - df[transaction_datetime]).dt.days)\n",
    "    return pd.concat([df,dummy],axis=1)\t\n",
    "\n",
    "####################################################################################################################################################################\n",
    "\n",
    "########################################################################################################################################################################\n",
    "def encodeToNumeric(df):\n",
    "#   print(df.select_dtypes(include=['object']).columns)\n",
    "    for col in df.select_dtypes(include=['object']):\n",
    "        enc = LabelEncoder()\n",
    "        enc.fit(df[col].astype('str'))\n",
    "        df[col] = enc.transform(df[col].astype('str'))\n",
    "    return df\n",
    "\n",
    "#########################################################################################################################################################################\n",
    "## Convert to datetime ##\n",
    "def date_time(df):\n",
    "    for col in date_col:\n",
    "        print(col)\n",
    "        df[col] = pd.to_datetime(df[col],format='%d/%m/%Y %H:%M:%S:%f')\n",
    "######################################################################################################################################################\n",
    "## Read masterfeed.xml ##\n",
    "import xml.etree.cElementTree as et\n",
    "def getvalueofnode(node):\n",
    "    \"\"\" return node text or None \"\"\"\n",
    "    return str(node) if node is not None else None\n",
    "\n",
    "def xmlExtractor():\n",
    "    \"\"\" XML Extractor \"\"\"\n",
    "    parsed_xml = et.parse(\".xml\")\n",
    "    dfcols = ['description', 'id_', 'isAvailableInSet', 'technicalName','transactionLoggingWriteMode','tuningOnDemandWriteMode','dataType']\n",
    "    df_xml = pd.DataFrame(columns=dfcols)\n",
    " \n",
    "    for node in parsed_xml.getroot():\n",
    "        description = node.attrib.get('description')\n",
    "        id_ = node.attrib.get('id')\n",
    "        isAvailableInSet = node.attrib.get('isAvailableInSet')\n",
    "        technicalName = node.attrib.get('technicalName')\n",
    "        transactionLoggingWriteMode = node.attrib.get('transactionLoggingWriteMode')\n",
    "        tuningOnDemandWriteMode = node.attrib.get('tuningOnDemandWriteMode')\n",
    "        dataType = node.attrib.get('type')\n",
    " \n",
    "        df_xml = df_xml.append(\n",
    "            pd.Series([getvalueofnode(description), getvalueofnode(id_), getvalueofnode(isAvailableInSet),\n",
    "                       getvalueofnode(technicalName),getvalueofnode(transactionLoggingWriteMode),getvalueofnode(tuningOnDemandWriteMode),\n",
    "                       getvalueofnode(dataType)\n",
    "                      ], index=dfcols),\n",
    "            ignore_index=True)\n",
    " \n",
    "    return df_xml\n",
    "\n",
    "######################################################################################################################################################\n",
    "## Unique Character Count ##\n",
    "def uniqueCharacterCount(df):\n",
    "    df_ = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        uniqueCharCount = df[col].drop_duplicates().apply(lambda x: len(str(x)))\n",
    "        df_[col] = uniqueCharCount\n",
    "\n",
    "######################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use below code snippet "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Dark borders ###\n",
    "#boder-style : hidden || default(restore)#\n",
    "%%HTML\n",
    "<style type=\"text/css\">\n",
    "table.dataframe td, table.dataframe th {\n",
    "    border-style: solid;\n",
    "}\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "df = train[list(col for col in float64 if col not in ['c5','c6','c7'])]\n",
    "# Two pass clustering\n",
    "# 1-We cluster the corr matrix\n",
    "#   We sort the survey data according to this clustering\n",
    "# 2-For cluster bigger than a threshold we cluster those sub-clusters\n",
    "#   We sort the survey data according to these clustering\n",
    "\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "cluster_th = 4\n",
    "\n",
    "X = df.corr().values\n",
    "d = sch.distance.pdist(X)\n",
    "d = np.where(np.isnan(np.array(d)),0,d)\n",
    "L = sch.linkage(d, method='complete')\n",
    "ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n",
    "\n",
    "columns = [df.columns.tolist()[i] for i in list(np.argsort(ind))]\n",
    "df = df.reindex_axis(columns, axis=1)\n",
    "\n",
    "unique, counts = np.unique(ind, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "columns = []\n",
    "for cluster_l1 in set(sorted(ind)):\n",
    "    j += counts[cluster_l1]\n",
    "    sub = df[df.columns.values[i:j]]\n",
    "    if counts[cluster_l1]>cluster_th:        \n",
    "        X = sub.corr().values\n",
    "        d = sch.distance.pdist(X)\n",
    "        d = np.where(np.isnan(np.array(d)),0,d)\n",
    "        L = sch.linkage(d, method='complete')\n",
    "        ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n",
    "        col = [sub.columns.tolist()[i] for i in list((np.argsort(ind)))]\n",
    "        sub = sub.reindex_axis(col, axis=1)\n",
    "    cols = sub.columns.tolist()\n",
    "    columns.extend(cols)\n",
    "    i = j\n",
    "df = df.reindex_axis(columns, axis=1)\n",
    "\n",
    "plot_corr(df, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                 Type        Data/Info\n",
      "----------------------------------------------\n",
      "Axes3D                   type        <class 'mpl_toolkits.mplot3d.axes3d.Axes3D'>\n",
      "datetime                 module      <module 'datetime' from '<...>onda3\\\\lib\\\\datetime.py'>\n",
      "decimal                  module      <module 'decimal' from 'C<...>conda3\\\\lib\\\\decimal.py'>\n",
      "detect_outliers          function    <function detect_outliers at 0x000002C7D270E7B8>\n",
      "glob                     module      <module 'glob' from 'C:\\\\<...>Anaconda3\\\\lib\\\\glob.py'>\n",
      "identical_columns        function    <function identical_colum<...>ns at 0x000002C7D270E620>\n",
      "np                       module      <module 'numpy' from 'C:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "os                       module      <module 'os' from 'C:\\\\Pr<...>\\\\Anaconda3\\\\lib\\\\os.py'>\n",
      "pd                       module      <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt                      module      <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "removeZeroVarianceData   function    <function removeZeroVaria<...>ta at 0x000002C7D270E950>\n",
      "sns                      module      <module 'seaborn' from 'C<...>s\\\\seaborn\\\\__init__.py'>\n",
      "time                     module      <module 'time' (built-in)>\n",
      "warnings                 module      <module 'warnings' from '<...>onda3\\\\lib\\\\warnings.py'>\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "## Look what is on memory ##\n",
    "%whos\n",
    "###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Unifing multiple Files #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extract fraud report from Directory and sub-directories  ####\n",
    "## on command promt ##\n",
    "!for /R C:\\XYZ\\ABC  %f in (*.csv) do copy %f C:\\EFG\\JKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where extracted fraud reports are present #\n",
    "path = r\"C:/EFG/JKL/\"\n",
    "all_files = glob.glob(os.path.join(path,\"*.csv\"))\n",
    "\n",
    "# concatenated Fraud report #\n",
    "df_from_each_file = (pd.read_csv(f,delimiter=',',header=0) for f in all_files)\n",
    "concatenated_df = pd.concat(df_from_each_file,ignore_index=True)\n",
    "\n",
    " \n",
    "\n",
    "# condition check for actimizeTransactionKey #\n",
    "\n",
    "if concatenated_df.columns[0] == 'UNIQUE_KEY':\n",
    "    concatenated_df.rename(columns={'UNIQUE_KEY':'uniqueKey'},inplace=True)\n",
    "\n",
    "# write on the disk #\n",
    "concatenated_df.to_csv(\"C:/EFG/JKL/_Unified_Report-2019-25-03.csv\",index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:00'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = pd.read_csv(\"C:/Users/sutiwari/Downloads/Cybage/video_streaming_data.csv\",\n",
    "                        delimiter=',',encoding='latin-1',header=0)\n",
    "df['TIMESTAMP'] = pd.to_datetime(df.UNIX_TIMESTAMP,unit='ms')\n",
    "#mergedCommercial_df.transactionNormalizedDateTime = pd.to_datetime(mergedCommercial_df.transactionNormalizedDateTime,format='%d/%m/%Y %H:%M:%S:%f')\n",
    "end = time.time() - start\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USERS_IP</th>\n",
       "      <th>UNIX_TIMESTAMP</th>\n",
       "      <th>X_PLAY_BACK_SESSION_ID</th>\n",
       "      <th>CHANNEL_ID</th>\n",
       "      <th>CHANNEL_NAME</th>\n",
       "      <th>VARIANT</th>\n",
       "      <th>SEGMENT_INDEX</th>\n",
       "      <th>CACHE_RESULT_CODE</th>\n",
       "      <th>TIME_TO_SERVER_MS</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.225.30.35</td>\n",
       "      <td>1535932476</td>\n",
       "      <td>02f7e781-f688-43aa-9cb1-633fb1e97022</td>\n",
       "      <td>11016</td>\n",
       "      <td>ABC KAKE</td>\n",
       "      <td>V5000_W</td>\n",
       "      <td>255988741</td>\n",
       "      <td>TCP_HIT</td>\n",
       "      <td>952</td>\n",
       "      <td>1970-01-18 18:38:52.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.225.30.35</td>\n",
       "      <td>1535932458</td>\n",
       "      <td>02f7e781-f688-43aa-9cb1-633fb1e97022</td>\n",
       "      <td>11016</td>\n",
       "      <td>ABC KAKE</td>\n",
       "      <td>V5000_W</td>\n",
       "      <td>255988738</td>\n",
       "      <td>TCP_HIT</td>\n",
       "      <td>894</td>\n",
       "      <td>1970-01-18 18:38:52.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.225.30.35</td>\n",
       "      <td>1535932620</td>\n",
       "      <td>02f7e781-f688-43aa-9cb1-633fb1e97022</td>\n",
       "      <td>11016</td>\n",
       "      <td>ABC KAKE</td>\n",
       "      <td>V5000_W</td>\n",
       "      <td>255988765</td>\n",
       "      <td>TCP_HIT</td>\n",
       "      <td>902</td>\n",
       "      <td>1970-01-18 18:38:52.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.225.30.35</td>\n",
       "      <td>1535932764</td>\n",
       "      <td>02f7e781-f688-43aa-9cb1-633fb1e97022</td>\n",
       "      <td>11016</td>\n",
       "      <td>ABC KAKE</td>\n",
       "      <td>V5000_W</td>\n",
       "      <td>255988789</td>\n",
       "      <td>TCP_HIT</td>\n",
       "      <td>888</td>\n",
       "      <td>1970-01-18 18:38:52.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.225.30.35</td>\n",
       "      <td>1535932673</td>\n",
       "      <td>02f7e781-f688-43aa-9cb1-633fb1e97022</td>\n",
       "      <td>11016</td>\n",
       "      <td>ABC KAKE</td>\n",
       "      <td>V5000_W</td>\n",
       "      <td>255988773</td>\n",
       "      <td>TCP_HIT</td>\n",
       "      <td>740</td>\n",
       "      <td>1970-01-18 18:38:52.673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       USERS_IP  UNIX_TIMESTAMP                X_PLAY_BACK_SESSION_ID  \\\n",
       "0  24.225.30.35      1535932476  02f7e781-f688-43aa-9cb1-633fb1e97022   \n",
       "1  24.225.30.35      1535932458  02f7e781-f688-43aa-9cb1-633fb1e97022   \n",
       "2  24.225.30.35      1535932620  02f7e781-f688-43aa-9cb1-633fb1e97022   \n",
       "3  24.225.30.35      1535932764  02f7e781-f688-43aa-9cb1-633fb1e97022   \n",
       "4  24.225.30.35      1535932673  02f7e781-f688-43aa-9cb1-633fb1e97022   \n",
       "\n",
       "   CHANNEL_ID CHANNEL_NAME  VARIANT  SEGMENT_INDEX CACHE_RESULT_CODE  \\\n",
       "0       11016     ABC KAKE  V5000_W      255988741           TCP_HIT   \n",
       "1       11016     ABC KAKE  V5000_W      255988738           TCP_HIT   \n",
       "2       11016     ABC KAKE  V5000_W      255988765           TCP_HIT   \n",
       "3       11016     ABC KAKE  V5000_W      255988789           TCP_HIT   \n",
       "4       11016     ABC KAKE  V5000_W      255988773           TCP_HIT   \n",
       "\n",
       "   TIME_TO_SERVER_MS               TIMESTAMP  \n",
       "0                952 1970-01-18 18:38:52.476  \n",
       "1                894 1970-01-18 18:38:52.458  \n",
       "2                902 1970-01-18 18:38:52.620  \n",
       "3                888 1970-01-18 18:38:52.764  \n",
       "4                740 1970-01-18 18:38:52.673  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[(df.CACHE_RESULT_CODE =='TCP_HIT')&(df.VARIANT=='V5000_W')].groupby([df.TIMESTAMP.dt.hour]).agg({'USERS_IP':lambda x:x.count()/50}).plot(kind='bar', figsize=(15,7),\n",
    "                                        color=\"Blue\", fontsize=13)\n",
    "\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"% HIT by hour of day - V5000_W\", fontsize=18)\n",
    "ax.set_ylabel(\"HIT %\", fontsize=18);\n",
    "ax.set_yticks([0])\n",
    "\n",
    "# create a list to collect the plt.patches data\n",
    "totals = []\n",
    "\n",
    "# find the values and append to list\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "\n",
    "# set individual bar lables using above list\n",
    "total = sum(totals)\n",
    "\n",
    "# set individual bar lables using above list\n",
    "for i in ax.patches:\n",
    "    # get_x pulls left or right; get_height pushes up or down\n",
    "    ax.text(i.get_x(), i.get_height()+.0006, \\\n",
    "            str(round((i.get_height()/total)*100, 2))+'%', fontsize=12,\n",
    "                color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
